
Nesse capítulo, será apresentada a construção do operador grosso Multiescala e suas aplicações. Daqui para frente, o grid original do problema ser denominado grid fino e o operador associado a esse grid $\rigidmatrix$ será chamado de operador fino. A partir do grid fino será construído um grid grosso onde cada elemento é uma aglomeração de elementos do grid fino, o operador associado a esse grid será chamado de operador grosso $\rigidmatrixcoarse$.


O método \textit{Multiscale Finite Element Method} (MSFEM) foi introduzido por \citet{thomashou} e inicialmente foi aplicado para obter soluções aproximadas do grid fino através do grid grosso para casos de transferência de calor de materiais e meios porosos com propriedades aleatórias. A principal conclusão desse trabalho foi que o método MSFEM é capaz de reproduzir a heterogeneidade da solução através da construção das funções de base com solução de problemas locais baseados no operador original. Mais tarde, o método MSFEM se mostrou problemático para conservação de massa em problemas de transporte e, por isso, foram criado os métodos \textit{Mixed Multiscale Finite Element} (MMSFEM) em \citet{mixedmsfem} e  \textit{Multiscale Finite Volume Method} (MSFVM) em \citet{msfv} que garantem a conservação.


Apesar do método MSFV garantir a conservação da massa, as soluções do grid fino não se mostravam adequadas para todas as situações, assin, foi apresentado o método multiescala iterativo em \citet{iterativems} que ao invés de realizar uma aproximação para a solução fina, utiliza a solução do sistema grosso juntamente com uma relaxação do operador fino para resolver o sistema na malha fina através de iterações de Richardson. A convergência da solução depende da quantidade de vezes que uma relaxação do grid fino é aplicada e varia de problema para problema.


Apesar de todos esses estudos,  o método multiescala continuava sendo possível aplicada em problemas em que informações do grid são conhecidas para a construção do operador grosso o que torna uma desvantagem em relação ao multigrid. Essa desvantagem é parcialmente resolvida através do Multiescala algébrico (apresentado em \citet{msalgebrico}) que, de certa forma semelhante aos métodos multigrid, montam o operador grosso através das entradas da matriz do grid fino utilizando uma ordenação de Wire-Basket que guarda a topologia da malha em questão. Métodos com vários níveis também já foram desenvolvidos como os \citet{multilevel}.


Os trabalhos citados nos parágrafos anteriores são de aplicações do método multiescala para os problemas de fluxo que, atualmente, na industria tem maior apelo por conta da previsão de produção dos campos. Porém, estudos relacionados ao método multiescala para geomecânica também podem ser encontrados em \citet{casteletto}, \citet{irina} e \citet{castelettoacoplado}. Além disso, visto que \eqref{eq:edp_geomec} é similar ao problema de elasticidade linear para sólidos, trabalhos como \citet{mbuck}. É importante salientar que \cite{casteletto} e \cite{irina} apresentam acoplamento entre a simulação de fluxo com a geomecânica.

Implementações em paralelo do método multiescala também podem ser encontradas, por exemplo em \citet{msparalelo} que faz comparação com o método multigrid mostrando que o multiescala é uma opção competitiva e tem escalabilidade similar ao multigrid.

\section{Construção de Operador Grosso Multiescala}

A ideia do método consiste na construção de um operador grosso semelhante ao realizado no multigrid de forma que a solução desse operador pode ser utilizada para ajudar na solução da escala original do problema ou ainda ser utilizada como uma aproximação para a mesma. Para isso, será necessário construir o próprio operador grosso e também operadores que fazem a transferência de escala do grid fino para o grosso (restrição) e do grid grosso para o grid fino (prolongamento).


O primeiro passo para a construção do operador multiescala é gerar um novo grid com menos elementos que o grid original do problema (grid fino), mas que ainda represente o mesmo domínio $\Omega$. Esse novo grid será chamado de grid grosso e as variáveis relacionadas com ele serão assinadas com o sobrescrito $H$.
Dessa forma, o grid grosso possui um conjunto $\coarseelementset$ de elementos onde cada elemento será uma aglomeração de elementos do grid fino. Por exemplo, a Figura \ref{fig:gridgrosso} apresenta um grid grosso $3\times 3$ construído a partir de um grid fino $7\times 7$.  As linhas finas representam as fiviões do grid fino e as grossas as divisões do grid grosso, já os  quadrados azuis destacam os nós que pertencem ao grid grosso e ao grid fino simultaneamente. Apesar de serem mostrados quantidade de diferentes quantidade de elementos sendo aglomerados para formar o grid grosso, as implementações desse trabalho se concentraram em juntar uma quantidade fixa de elementos em cada direção. Define-se o fator de engrossamento de acordo com \eqref{eq:fatordeengrossamento}. Analogamente podem ser definidos os fatores $\coarsefactorx$ e $\coarsefactory$ que representam os engrossamento em cada direção, onde, $\coarsefactor = \coarsefactorx \times \coarsefactory$.

\begin{figure}[!htbp]
\centering
\includegraphics[width=6cm]{chap06/figs/grosso.png}
\caption{Exemplo de grid fino $7\time 7$ e um grid grosso $3\times 3$. O elemento inferior esquerdo é composto por 9 elementos do grid fino enquanto o elemento superior direito é composto com 4 elementos do grid fino.}
\label{fig:gridgrosso}
\end{figure}


\begin{equation} \label{eq:fatordeengrossamento}
    \coarsefactor = \frac{\numelementsfine}{\numelementscoarse}
\end{equation}




Do mesmo modo que apresentado para o grid fino no Capítulo \ref{ch:discretizacao}, cada grau de liberdade grosso $\freedomcoarse$ será associada uma função de base $\basefunctioncoarse$ e cada nó associado a uma condição de contorno de Dirichlet $\essentialcoarse$ uma função de base $\basefunctioncoarseessential$. E então, é possível encontrar uma solução aproximada do grid fino $\finesolution \approx \msfinesolution$ através de \eqref{eq:aproximacaogrossa}.

\begin{equation} \label{eq:aproximacaogrossa}
    \msfinesolution = \coarsesolution = \sum_{i=1}^{\qtdfreedomcoarse} \basefunctioncoarse \freedomcoarse + {\sum_{i=1}^ {\qtdfreedomcoarse}}  \basefunctioncoarseessential \essentialcoarse
\end{equation}


Diferentemente do caso fino, as funções $\basefunctioncoarse$ serão uma combinação linear das funções $\basefunctionfine$ calculadas a partir de problemas homogêneos locais  para cada elemento $\coarseelement \in \coarseelementset$  mostrado em \eqref{eq:problemaslocais}, conforme mostrado em \cite{casteletto}.


\begin{subequations} \label{eq:problemaslocais}
\begin{align}
\sopnabla ^T (D \sopnabla \basefunctioncoarse)  = \mathbf{0} \text{,   em   } \coarseelement \label{eq:operadorlocal}\\
\sopnablafront ^ T ( D  \sopnablafront \basefunctioncoarse) = \mathbf{0}  \text{,   em   } \partial \coarseelement  \label{eq:operadoraresta}\\
\basefunctioncoarse(\mathbf{x}_j) = \delta_{ij} \mathbf{e}
\label{eq:valorvertice}
\end{align}
\end{subequations}

onde $\sopnablafront$ está definido em \eqref{eq:sopnabladef} para as coordenadas $\hat{x}$ e $\hat{y}$ que representam a direção paralela e perpendicular a $\partial \coarseelement$ como mostrada na Figura \ref{fig:direcoesoperadorfronteira}, $\mathbf{x}_j$ representa as coordenadas do vértice relacionado ao grau de liberdade $\freedomcoarse[j]$ e  $\mathbf{e} = [1\quad0]^T $ se $\basefunctioncoarse$ é relacionada a um grau de liberdade x caso contrário,  $\mathbf{e}=[0\quad1]^T$.

\begin{equation}\label{eq:sopnabladef}
\sopnablafront = \begin{bmatrix}
\dxhat  & 0 \\ 
0 & 0 \\ 
0 & \dxhat 
\end{bmatrix}
\end{equation}


%TODO fazer a minha própria figura. Por enquanto está a mesma do casteletto.
\begin{figure}[!htbp]
\centering
\includegraphics[width=11cm]{chap06/figs/direcoesoperadorfronteira.png}
\caption{Direções para definir operador dos problemas locais na fronteira. Adaptado de \cite{casteletto}.}
\label{fig:direcoesoperadorfronteira}
\end{figure}


Um fato importante é que \eqref{eq:operadorlocal} mostra que a função de base $\basefunctioncoarse$ satisfaz ao mesmo operador que o problema fino, de acordo com \citet{thomashou} essa é uma das vantagens do método multiescala, pois as funções de base tentam reproduzir o operador localmente. Essa vantagem vem com o custo da solução dos problemas locais que também de acordo com \citet{thomashou} são custosas. Estimativas do dispêndio desse cálculos são encontradas em \citet{msparalelo} e \citet{mbuck}.

Dessa forma, desconsiderando as condições de contorno, dado um elemento quadrilátero $\coarseelement$, este possui oito funções $\basefunctioncoarse$ que são não nulas nesse domínio (duas para cada um dos vértices). Considerando uma numeração local do elemento e representando as funções como $\basefunctionelemcoarse$ para $i=1,2,\dots, 8$. A Figura \ref{fig:coarsefunctionslocalnum} apresenta as funções associadas a cada um dos nós na numeração local.


\begin{figure}[!htbp]
\centering
\includegraphics[width=8cm]{chap06/figs/funcoesDeBaseGrossasColorido.png}
\caption{Funções de base associadas a cada um dos nós de um elementos quadrilátero. As cores verdes se referem as funções associadas a x e vermelho a y. As setas representam o valor da função no seu vértice correspondente \eqref{eq:valorvertice}.}
\label{fig:coarsefunctionslocalnum}
\end{figure}

Para cada uma das funções é necessário resolver \eqref{eq:problemaslocais}, a solução é dividida em duas etapas, primeiro é resolvido \eqref{eq:operadoraresta} utilizando como condição de contorno \eqref{eq:valorvertice}, a resposta representa os valores de $\basefunctionelemcoarse$ na fronteira do elemento $\coarseelement$. Essa etapa pode ser realizada utilizando o  método dos elementos finitos onde a matriz relativa a cada elemento, que nesse caso são segmentos de reta, podem ser encontrados no apêndice de \cite{casteletto}. 


Com os valores para  fronteira $\coarseelement$  definidos, eles são utilizados como condição de contorno de Dirichlet para \eqref{eq:operadorlocal}. Esse problema também é resolvido com elementos finitos onde os resultados encontrados serão chamados de coeficientes $p_{ij}$ que descrevem $\basefunctioncoarse$ como combinação linear de $\basefunctionfine$ conforme mostrado em \eqref{eq:linearbasecoarse}. 


\begin{equation} \label{eq:linearbasecoarse}
    \basefunctioncoarse = \sum_{j=1}^{\qtdfreedomfine} p_{ji} \basefunctionfine[j]
\end{equation}


É importante perceber que os coeficientes $p_{ji}$ fazem uma associação entre o espaço grosso e fino. A matriz $\mathbf{P}$ que será o operador de prolongamento terá justamente esses coeficientes como entradas. De forma que, $[\mathbf{P}]_{i,j} = p_{ij}$ e $\mathbf{P}$ tem dimensão $\qtdfreedomcoarse \times \qtdfreedomcoarse$.

%TODO adicionar explicação sobre os valores de 


Com \eqref{eq:linearbasecoarse} as funções de base do grid grosso estão determinadas e então é possível formular um novo problema \eqref{eq:weakcoarseformaprox} com espaços $\trialcoarseaprox$ e $\testcoarseaprox$  do grid grosseiro de maneira análoga a realizada em \eqref{eq:weakformaprox}.



\begin{empheq}[box=\mymath]{equation}\label{eq:weakcoarseformaprox}
\begin{split}
\text{Encontrar  }  \mathbf{u} \in \testcoarseaprox \text{ tal que} \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \\
\omeint{ (\sopnabla \mathbf{w})^T D \sopnabla  \mathbf{u}} - \int_{\Gamma_\sigma} \mathbf{w} \bar{\mathbf{t}} d\Gamma = (\sopnabla\mathbf{w})^T m P_p \quad \forall \mathbf{w} \in \trialcoarseaprox
\end{split}
\end{empheq}

onde $\trialcoarseaprox =  \text{span}\{ \basefunctioncoarse[1], \basefunctioncoarse[2], \hdots, \basefunctioncoarse[\qtdfreedomcoarse] \}$ e $\testcoarseaprox =  \text{span}\{ \basefunctioncoarse[1], \basefunctioncoarse[2], \hdots, \basefunctioncoarse[\qtdfreedomfine + \qtdfreedomcoarse] \}$. Portanto, a matriz de rigidez relativa ao grid grosso é dada por tem entradas conforme \eqref{eq:entradamatrizgrossa}.

\begin{equation} \label{eq:entradamatrizgrossa}
    \rigidmatrixcoarse_{i,j} =\omeint{(\sopnabla \basefunctioncoarse[i])^T D \sopnabla \basefunctioncoarse[j]}
\end{equation}

Substituindo \eqref{eq:linearbasecoarse} em \eqref{eq:entradamatrizgrossa}.


\begin{align}
     K^H_{i,j}  & =   \omeint{(\sopnabla \basefunctioncoarse[i])^T D \sopnabla \basefunctioncoarse[j]} \nonumber \\
                & =   \omeint{(\sopnabla  \sum_{k=1}^{\qtdfreedomfine} p_{ki} \basefunctionfine[k])^T D \sopnabla( \sum_{l=1}^{\qtdfreedomfine} p_{lj} \basefunctionfine[l] )}  \nonumber \\
                & =    \sum_{k=1}^{\qtdfreedomfine}  \sum_{l=1}^{\qtdfreedomfine} \omeint{p_{ki} ( \sopnabla \basefunctionfine[k])^T D \sopnabla \basefunctionfine[l] p_{lj}  }                  \nonumber \\
                & =    \sum_{k=1}^{\qtdfreedomfine}  \sum_{l=1}^{\qtdfreedomfine} p_{ki} \left( \omeint{ ( \sopnabla \basefunctionfine[k])^T D \sopnabla \basefunctionfine[l]   } \right)p_{lj}                            \nonumber \\
                & = \omeint{     p_{ki} \rigidmatrix_{kl} p_{lj}}  \nonumber \\
                & = \omeint{   [P]_{ki} \rigidmatrix_{kl} [P]_{lj}}                                         \nonumber \\
                & = \omeint{   [P^T]_{ik} \rigidmatrix_{kl} [P]_{lj}     }   \label{eq:ptapeinstein}  \\
\end{align}


Portanto, a \eqref{eq:ptapeinstein} mostra que o operador grosseiro pode ser construído utilizando \eqref{eq:ptap} diferentemente de realizar loop nos elementos grossos para como o realizado pelo método de elementos finitos clássico.

\begin{equation} \label{eq:ptap}
    \mathbf{K}^H = \mathbf{P}^T  \mathbf{K}^h \mathbf{P}
\end{equation}

De forma análoga pode ser encontrar que a relação entre os lados direito dos dois operador é $\mathbf{f}^H = \mathbf{P}^T \mathbf{f}^h$. Com isso, dado um sistema linear da forma apresentada em \eqref{eq:sistemalinear} é possível encontrar  \eqref{eq:aproximacaogrossa2} como aproximação para a solução do grid fino.

\begin{equation} \label{eq:aproximacaogrossa2}
    d^h \approx d^h_{ms} = \mathbf{P} (\mathbf{K}^H)^{-1} \mathbf{P}^{T} \mathbf{f}^h
\end{equation}

Essa aproximação consiste de três passos que são descritas abaixo:

\begin{itemize}
    \item A multiplicação por $\mathbf{P}^T$ representa a restrição do lado direito para o espaço grosseiro.
    \item  o termo $(\rigidmatrixcoarse)^{-1}$ representa o cálculo da solução do espaço grosseiro.
    \item a multiplicação por $\mathbf{P}$ é o prolongamento da solução do espaço grosseiro para espaço fino.
\end{itemize}


Inicialmente, a solução aproximada \eqref{eq:aproximacaogrossa} foi utilizada como solução aproximada para o grid fino em \cite{thomashou}. No caso, tomando como base \eqref{eq:aproximacaogrossa2}, pode-se definir $\preconms = \mathbf{P} (\rigidmatrixcoarse)^{-1} \mathbf{P}^{T}$ como um precondicionador multiescala. Em \citet{zhouiterativo}  essa solução foi utilizada como solver através iterações de Richardson junto com um pré-condicionador do espaço fino, pois mostra que esse pré-condicionador não pode ser utilizado sozinho devido a sua convergência não ser garantida visto que a matriz $\rigidmatrixcoarse$ é deficiente de rank e portanto a matriz de  iteração tem autovalores iguais a um. No caso, o pré-condicionador utilizado para solução foi um pré-condicionador {\color{red} combinativo multiplicativo} $\precon=\preconmult$ onde $\preconfine$ foi utilizado o BILU. Visando reproduzir os resultados para os operadores de elasticidade linear, a Figura \ref{fig:autovaloresRichard} apresenta os autovalores relativos a um caso homogêneo de um grid $40\times40$ para uma iteração de Richardson para quatro diferentes pré-condicionadores: multiescala, ILU(0), combinativo multiplicativo (multiescala + ILU(0)) e combinativo aditivo (multiescala + ILU(0)). Ainda sobre o fato do operador ser utilizado conjuntamente com um pré-condicionador fino, essa é a abordagem típica e também é utilizada em \cite{msparalelo} e \cite{casteletto}.


\begin{figure}[h]
\center
\subfigure[Multiescala $4\times4$ ]{\includegraphics[width=0.45\textwidth]{chap06/figs/autovalores_richardson_mms.png}}
\qquad
\subfigure[ ILU(0) ]{\includegraphics[width=0.45\textwidth]{chap06/figs/autovalores_richardson_ilu.png}}

\subfigure[ Aditivo (Multiescala + ILU(0)) ]{\includegraphics[width=0.45\textwidth]{chap06/figs/autovalores_richardson_aditivo.png}}
\qquad
\subfigure[ Multiplicativo (Multiescala + ILU(0)) ]{\includegraphics[width=0.45\textwidth]{chap06/figs/autovalores_richardson_multiplicativo.png}}
\caption{Autovalores para matriz de iteração de Richardson para caso homogêneo e isotrópico. Grid de tamanho $40\times40$. }\label{fig:autovaloresRichard}
\end{figure}



Pode-se verificar que o pré-condicionador multiescala não tem convergência garantida, em concordância com a prova mostrada em \cite{zhouiterativo}. O combinativo multiplicativo apresenta bom resultado pois os autovalores aparecem próximos de zero, entretanto, o combinativo aditivo não tem convergência garantida o que mostra que não é interessante utiliza-lo com iterações de Richardson. Ainda sobre isso, \cite{casteletto} mostra que o operador multiplicativo tem melhor desempenho quando utilizado junto com um método de Krylov, naquele contexto o Bicgstab, do que quando utilizado com o método de Richardson, inclusive, mostra que para várias configurações o solver não consegue convergir. Dado esse resultado, o presente trabalho vai fazer comparações entre os métodos utilizando métodos de Krylov. Esses costumam funcionar melhor quanto mais clusterizados estão os autovalores da matriz {\color{red}(referencia para isso ?)}. A Figura \ref{fig:autovaloresMatrizPrecon} apresenta os valores da matriz de rigidez pré-condicionada ($\precon \rigidmatrix$) para o mesmo grid de $40 \times 40$, os gráficos apresentam  resultados para engrossamento 2x2, 4x4 e 8x8, todas as figuras são apresentadas com os mesmos eixos para ser possível se observar as espalhamento dos autovalores.

É fato que nos dois casos o aumento do engrossamento causa um maior espalhamento dos autovalores, desse modo é de se esperar que com isso sejam necessárias mais iterações para a convergência do solver. Além disso, o pré-condicionador aditivo possui distribuição pior dos autovalores em relação ao multiplicativo e, portanto, é esperado que utilize mais iterações. Esse aumento tem que ser compensado com uma iteração mais barata por conta da multiplicação matriz vetor.




\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth]{chap06/figs/AutovaloresMatPrecondicionada.png}
\caption{Autovalores da matrix pré-condicionada para caso homogêneo.}
\label{fig:autovaloresMatrizPrecon}
\end{figure}



\section{Construção do Operador de Prolongamento}

A solução dos problemas locais descritos na seção anterior tem como resultado as entradas $p_{ij}$ do operador em cada um dos elemento de $\coarseelementset$. Para encontrar esses valores é necessário primeiro construir a matriz de rigidez relativa ao elemento em questão que denominaremos de $\rigidmatrixelementms$. Essa matriz será utilizada para a solução de todas as oito funções de base relacionadas com esse elemento, uma maneira de adicionar mais facilmente as condições de contorno nesse caso é utilizar o método de condição de contorno apresentado em \eqref{eq:diagident}, dessa forma, o lado direito do sistema conterá apenas os valores da condição de contorno de Dirichlet.

\vspace{1cm}
\noindent\fbox{%
    \parbox{\textwidth}{%
        \begin{algorithmic}
        \STATE Inicialize $\mathbf{P}$ com zeros
        \FORALL{elemento $\coarseelement \in \tau^H$}
        \STATE Calcular Matriz de Rigidez $\rigidmatrixelementms$ do Problema Local de $\coarseelement$ 
        \FOR{$i=1,\hdots,8$}
        \STATE Resolver \eqref{eq:operadoraresta} em  $\partial \coarseelement$ com condição de contorno \eqref{eq:valorvertice}
        \STATE Construa $\mathbf{f}^{\coarseelement}$ com valores do problema na fronteira.
        \STATE $\mathbf{P_{\text{local}}} \leftarrow  (\rigidmatrixelementms)^{-1} \mathbf{f}^{\coarseelement} $
        \STATE Atribua em $\mathbf{P}$ o valores $\mathbf{P_{\text{local}}}$ nas posições correspondentes
        \ENDFOR
        \ENDFOR
        \end{algorithmic}
    }%
}
\vspace{1cm}



Com o algoritmo acima é possível obter $\mathbf{P}$ e, com ele, obter $\rigidmatrixcoarse$ através de \eqref{eq:ptap} e a construção do pré-condicionador $\preconms = \mathbf{P} (\mathbf{K}^H)^{-1} \mathbf{P}^{T}$ fica completa. 


%O primeiro passo é a construção de um grid grosso a partir do grid fino. A nomenclatura dada para os elementos do grid grosso serão direnciadas da do grid fino pelo sobreescrito H maiúsculo. Isso será feito de forma que cada elemento do grid grosso seja uma aglomeração de elementos do grid fino. Por exemplo, a Figura \ref{fig:gridgrosso} mostra um grid fino 7x7 que foi transformado em um grid grosso 3x3 através da junção de elementos. Nesse caso, o grid grosso é formado por nove elementos e dezesseis nós. Do mesmo que apresentado no Capítulo \ref{ch:discretizacao}, cada grau de liberdade do grid grosso $\freedomcoarse$ terá uma função de base associada  $\basefunctioncoarse$.



%Ele consiste basicamente em construir um operador grosso através do cálculo de funções de base em um grid gerado pelo acoplamento de elementos do grid fino. Pode ser utilizado como pré-condicionador \cite{casteletto}, como solver multinível semelhante aos solver multigrid ou ainda como aproximação para a solução original do problema. Os métodos multiescala tem sido aplicados com sucesso para problemas elípticos que é o caso do problema da elasticidade linear apresentado aqui. As vantagens do método discutido em \cite{thomashou} são que as funções de base multiescala tentam se adaptar às propriedades locais do operador, de forma que o operador grosso as conserve. As funções de base podem ser construídas através da solução de problemas independentes e, portanto, em paralelo.







\section{Cálculo do NNZ do operador de prolongamento}\label{sec:complexProlong}

Em um método multiescala, além da construção de um operador grosso,  precisa-se mover os vetores entre os espaços fino e grosso  Essas movimentações são realizadas através de multiplicação por operadores especiais, $P$ e $P^T$. Essa seção descreve a complexidade destas operações.

As dimensões do operador de prolongamento dependem dos graus de liberdade do operador fino e do  grosso, valendo $\qtdfreedomfine \times \qtdfreedomcoarse$. Conforme visto no Capítulo \ref{ch:sistemas}, a multiplicação de uma matriz esparsa por um vetor é da ordem do número de elementos  não nulos da matriz, assim, apesar de um maior engrossamento do grid reduzir o valor de $n_u^H$,  não necessariamente ocorre  uma redução de elementos não nulos de $P$. \suge{Talvez uma forma simples de motivar esta ideia, seja dizer que todos os elementos a matriz $A$ tem que ser multiplicados por algum elemento da matriz de restrição, não podendo ficar nenhum de fora, sendo assim, mesmo que ser reduza o número de linhas do operador de restrição, a quantidade de elementos não nulos não pode ser alterada. O que ocorre é que cada linha do operador de restrição, ou coluna do de extensão, fica mais densa quando se diminui a dimensão do espaço grosso}

Considerando um grid fino com $\numelementsxfine \times \numelementsyfine$, um grid grosso onde cada elemento tem dimensões $q_x \times q_y$ \suge{neste caso você está considerando que os elementos grossos são formados sempre pelo mesmo número de elementos, talvez seja o caso de dizer que você vai fazer isso mais lá em cima, apesar de isso não ser necessário} e, ainda, que mesmo os nós de fronteira com condição de contorno de Dirichlet não são removidos da matriz de rigidez, então a quantidade de não zeros do operador de prolongamento ($nnz_p$) pode ser calculada contando os nós de três conjuntos:  nós interiores, nós no vértice e os nós na aresta.  A Figura \ref{fig:gridCompleto} mostra cada um desse tipo de nós (em vermelho nó no vértice, em azul nó na aresta e em preto nó interior).


\begin{figure}[h]
\center
\subfigure[  ]{\includegraphics[width=0.45\textwidth]{chap06/figs/gridCompleto.jpeg}\label{fig:gridCompleto}}
\qquad
\subfigure[ ]{\includegraphics[width=0.45\textwidth]{chap06/figs/noInterior.jpeg}\label{fig:noInterior}}
\subfigure[ ]{\includegraphics[width=0.45\textwidth]{chap06/figs/noVertice.jpeg}\label{fig:noVertice}}
\subfigure[ ]{\includegraphics[width=0.45\textwidth]{chap06/figs/noAresta.jpeg}\label{fig:noAresta}}

\caption{Comparação da solução do grid fino com a solução do grid grosso.  }
\label{fig:verticesTypes}
\end{figure}

A quantidade de cada um desses nós na malha grossa é apresentado abaixo.

\begin{itemize}
    \item Nós interiores: $(\frac{n_x}{q_x} - 1) (\frac{n_y}{q_y} - 1)$
    \item Nós de aresta em $\Gamma_l$ e $\Gamma_r$: $2 ( \frac{n_y}{q_y} - 1)$
    \item Nós de aresta em $\Gamma_t$ e $\Gamma_b$: $2 ( \frac{n_x}{q_x} - 1)$
    \item Nós vértices: 4
\end{itemize}


Cada nó tem duas funções de base associadas uma para cada grau de liberdade (x e y). Para construirmos o operador de prolongamento que relaciona os nós do grid grosso com o fino, temos que a cada nó do grid grosso tem relação com $(2q_x+1)(2q_y+1)$ do grid fino. Este será o número de entradas não nulas que aparecerão em cada coluna do operador de prolongamento, como mostra a Figura \ref{fig:noInterior}. Além disso, ela afeta os dois graus de liberdade de cada um desses nós, então cada um deles contribui com $4(2q_x+1)(2q_y+1)$  não zeros para o prolongamento (uma implementação mais cuidadosa do método pode economizar as bordas do suporte pois as funções se anulam). De maneira análoga, os nós vértices contribuem com $ 4 (q_x+1)(q_y+1)$, os nós aresta em $\Gamma_t$ e $\Gamma_b$ contribuem com $ 4 (2q_x+1)(q_y+1) $ e, finalmente, os nós arestas  $\Gamma_l$ e $\Gamma_r$ e contribuem com $ 4(q_x+1)(2q_y+1) $. Assim, para encontrar a quantidade total de não zeros do operador de prolongamento, basta multiplicar as quantidades de cada um dos nós pela duas contribuições conforme a   \eqref{eq:nnzRaw}.


\begin{equation} \label{eq:nnzRaw}
\begin{aligned}
    nnz_P = & 4(2q_x+1)(2q_y+1)  (\frac{n_x}{q_x} - 1) (\frac{n_y}{q_y} - 1)   \\
            & + 4 (q_x+1)(2q_y+1)  2 ( \frac{n_y}{q_y} - 1) +  4 (2q_x+1)(q_y+1)  2 (\frac{n_x}{q_x} - 1) \\
            & +  4(q_x+1)(q_y+1) 4
\end{aligned}
\end{equation}

Que pode ser modificada para a \eqref{eq:nnzRaw2},

\begin{equation} \label{eq:nnzRaw2}
\begin{aligned}
    nnz_P = &   4 (2+\frac{1}{q_x})(2 + \frac{1}{q_y})  (n_x - q_x) (n_y - q_y) \\
            & + 8 (q_x+1)(2 + \frac{1}{q_y})  (n_y - q_y) \\
            & + 8 (2+\frac{1}{q_x})(q_y+1) (n_x - q_x) \\
            & + 16 (q_x+1)(q_y+1)
\end{aligned}
\end{equation}

Assim, se $q_x$ e $q_y$ são de ordem $O(1)$ o primeiro termo da soma é da ordem de $O(n_x \times n_y)$. Caso $q_x$ e $q_y$ sejam de ordem $O(n_x)$ e $O(n_y)$, então o último termo da soma é da ordem $O(n_x \times n_y )$. \suge{não vejo muito sentido em $q_x$ ser $O(1)$, pois $q_x$ e $n_x$ vão sempre ter uma relação direta e, neste caso, o conceito de ordem esconde a constante de proporcionalidade e não ajuda a definir bem a complexidade do problema. $q_x$ e $n_x$ não teriam relação caso um fosse fixado e outro alterado, que é a situação mostrada na Figura \ref{fig:nnzGrafico}}
Se $q_x$ é $O(n_x)$ e $q_y$ é $O(1)$ então o segundo termo é de ordem $O(n_x \times n_y)$, analogamente para o caso contrário.
De toda forma, a quantidade de não zeros do prolongamento é da ordem do tamanho do grid $n_x \times n_y$ que é um fato importante, pois a multiplicação pelo prolongamento faz parte do processo de utilização do pré-condicionador multiescala e esse preço sempre terá que ser pago. O valor limite também dos não zeros é representado pelo último termo da soma $16(q_x+1)(q_y+1)$.

Um gráfico representando    \eqref{eq:nnzRaw2} é mostrado na Figura \ref{fig:nnzGrafico}, no eixo y é apresentado $nnz_P$ enquanto no eixo x é apresentado o engrossamento da malha que tem a mesma proporção em x e y ($q_x  = q_y$) para um grid de $1024 \times 1024$. Pelo gráfico é possível ver que o $nnz_P$ tende já em um engrossamento $16 \times 16$.


\begin{figure}[!htbp]
\centering
\includegraphics[width=8cm]{chap06/figs/nnzProlongamento.png}
\caption{Quantidade de não zeros do prolongamento $nnz_P$  em função do engrossamento da malha. A linha pontilhada mostra o valor de $16(n_x+1)(n_y+1)$.}
\label{fig:nnzGrafico}
\end{figure}
